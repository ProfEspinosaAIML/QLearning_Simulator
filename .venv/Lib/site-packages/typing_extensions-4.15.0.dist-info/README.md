# ğŸ¯ Q-Learning Simulator (Teaching Edition)

Welcome to the **Q-Learning Simulator â€“ Teaching Edition**.

This project is designed to help you **understand, experiment with, and visualize Q-Learning** as presented in:

> Sutton & Barto, *Reinforcement Learning: An Introduction (2nd Edition)*

The simulator is intentionally simple, tabular, and transparent â€” so you can see exactly how the algorithm works.

---

# ğŸ“š What You Will Learn

By running and modifying this project, you will learn:

- How the Q-Learning update works:
  
  \[
  Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]
  \]

- The difference between:
  - Exploration (Îµ-greedy behavior)
  - Exploitation (greedy policy)
  - Training vs Evaluation
  - Sparse vs Dense rewards
- How convergence emerges from repeated updates
- Why early success â‰  convergence

---

# ğŸ— Project Structure

Single-file implementation:

```
q_learning_simulator.py
```

Includes:

- Tabular Q-Learning agent
- FrozenLake (Gymnasium) support
- Built-in GridWorld
- Early stopping
- Logging
- Slow-motion visual play mode
- Model saving/loading

---

# ğŸš€ Installation

Create a virtual environment (recommended):

```bash
python -m venv .venv
source .venv/bin/activate   # Mac/Linux
.venv\Scripts\activate     # Windows
```

Install dependencies:

```bash
pip install numpy
pip install "gymnasium[toy-text]"
```

This installs:
- Gymnasium
- FrozenLake
- pygame (for rendering)

---

# ğŸ§ª Quick Start

## 1ï¸âƒ£ Train Silently (Fast)

```bash
python q_learning_simulator.py --env gym --gym-id FrozenLake-v1 --episodes 5000 --epsilon-decay
```

What you see:
- Training logs
- Final evaluation metrics

No graphics yet â€” just learning.

---

## 2ï¸âƒ£ Best Teaching Mode (Recommended)

Train, stop on convergence, then visually demonstrate learned policy:

```bash
python q_learning_simulator.py --env gym --gym-id FrozenLake-v1   --episodes 5000   --epsilon-decay   --log-every 10   --stop-avg-reward 0.7   --ma-window 100   --play-after-training 5   --render human   --play-sleep 0.15
```

What happens:

1. Console shows learning progress
2. Training stops when success rate â‰ˆ 70%
3. Then 5 greedy episodes are played slowly
4. You see the learned optimal path

This is the best way to understand convergence.

---

## 3ï¸âƒ£ Stop on First Success

```bash
python q_learning_simulator.py --env gym --gym-id FrozenLake-v1   --episodes 5000   --epsilon-decay   --stop-on-goal   --play-after-training 5   --render human
```

Good for illustrating that:

> One success does NOT mean the agent has converged.

---

## 4ï¸âƒ£ Live Training + Graphics (Exploration Demo)

```bash
python q_learning_simulator.py --env gym --gym-id FrozenLake-v1   --episodes 300   --epsilon-decay   --render human   --train-render-sleep 0.05   --log-every 1
```

This shows exploration in real-time.

Useful for explaining Îµ-greedy behavior.

---

# ğŸ§± Using the Built-In GridWorld

No Gym required:

```bash
python q_learning_simulator.py --env grid --episodes 2000 --epsilon-decay
```

Or with live visualization:

```bash
python q_learning_simulator.py --env grid   --episodes 300   --render human   --train-render-sleep 0.08
```

GridWorld is ideal for debugging and understanding mechanics.

---

# ğŸ” Key Concepts to Observe

While running experiments, pay attention to:

### Îµ (Epsilon)
- Starts high â†’ lots of exploration
- Gradually decays â†’ more exploitation

### Î± (Learning Rate)
- High Î± â†’ faster learning, more instability
- Low Î± â†’ slower, smoother convergence

### Î³ (Discount Factor)
- High Î³ â†’ long-term planning
- Low Î³ â†’ short-sighted behavior

Try modifying these and observe changes.

---

# ğŸ“Š Understanding the Logs

Example log line:

```
[episode    200] avg_reward=+0.320 avg_len=8.4 success_rate=32.00% eps=0.818
```

Meaning:

- Average reward over last N episodes
- Average episode length
- Success rate (goal reached)
- Current epsilon value

These tell you whether learning is improving.

---

# ğŸ’¾ Saving and Loading Models

Train and save:

```bash
python q_learning_simulator.py --env gym --gym-id FrozenLake-v1   --episodes 5000   --epsilon-decay   --save models/frozenlake_Q.npz
```

Then later:

```bash
python q_learning_simulator.py --env gym --gym-id FrozenLake-v1   --episodes 0   --load models/frozenlake_Q.npz   --play-after-training 10   --render human
```

This lets you demo without retraining.

---

# ğŸ§  Suggested Experiments

1. Remove epsilon decay â€” does it converge?
2. Set epsilon to 0 from the start â€” what happens?
3. Change Î³ to 0.5 â€” what changes?
4. Increase grid size â€” how does learning scale?
5. Compare stop-on-goal vs stop-avg-reward

---

# ğŸ“ How This Connects to Theory

This implementation directly matches the pseudocode in Sutton & Barto:

- Off-policy TD control
- Bootstrapped target
- Greedy target policy
- Îµ-greedy behavior policy

Everything is tabular â€” no function approximation â€” so the learning dynamics are visible and interpretable.

---

# ğŸ›  Troubleshooting

If graphics donâ€™t appear:

```bash
pip install "gymnasium[toy-text]"
```

If running in WSL or remote environment:
- Make sure GUI forwarding is enabled.

---

# âœ¨ Educational Philosophy

This simulator prioritizes:

- Clarity over abstraction
- Transparency over performance
- Observability over automation

The goal is not just to run Q-Learning â€” but to understand it.

---

# ğŸ“Œ Final Advice

Do not just run the code.

Change it.
Break it.
Plot it.
Instrument it.

Reinforcement Learning becomes intuitive only when you experiment.

---


## ğŸ”¥ Hyperparameter Challenge

In the simulator source code you will find three key hyperparameters:

```python
gamma: float = 0.99
alpha: float = 0.1
epsilon: float = 0.1
```

Your challenge:

Try to achieve a *more optimal and stable* learning run by experimenting with:

### 1ï¸âƒ£ Gamma (Î³) â€” Discount Factor
- What happens if you reduce it to 0.8?
- What if you increase it to 0.999?
- Does the agent become more short-sighted or more strategic?

### 2ï¸âƒ£ Alpha (Î±) â€” Learning Rate
- Try 0.5 â€” does learning become unstable?
- Try 0.01 â€” does convergence slow down?
- Can you find a â€œsweet spotâ€?

### 3ï¸âƒ£ Epsilon (Îµ) â€” Exploration Rate
- What happens if Îµ is too high?
- What if Îµ starts high but decays?
- What happens if Îµ = 0 from the beginning?

ğŸ¯ Your goal:

- Maximize success rate
- Minimize episode length
- Achieve stable convergence (not just one lucky success)

Document your experiments and explain **why** the behavior changes.

Understanding how these parameters shape learning dynamics is one of the most important practical skills in Reinforcement Learning.


Happy Learning ğŸš€
